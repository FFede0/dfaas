{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to fill NaN values within the dataframe X\n",
    "def fill_NaN(X):\n",
    "  for col in X:\n",
    "    X.loc[:, col] = X.loc[:, col].fillna(1)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method plot\n",
    "def plot_elbow(K, distortions):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distorsione')\n",
    "    plt.title('Il Metodo del Gomito')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSV = 'output/generator'\n",
    "FUNCTION_NAMES = ['figlet', 'shasum', 'nmap', 'env', 'curl', 'eat-memory']\n",
    "FUNCTION_NAMES_REDUCED = ['figlet', 'shasum', 'nmap', 'env', 'curl']\n",
    "COLUMNS_NAMES = ['name', 'rate', 'success_rate', 'cpu_usage', 'ram_usage', 'power', 'overloaded', 'medium_latency']\n",
    "COLUMNS_REDUCED = ['rate', 'success_rate', 'cpu_usage', 'ram_usage', 'power', 'overloaded', 'medium_latency']\n",
    "COLUMNS_SCALED = ['rate', 'success_rate', 'cpu_usage', 'ram_usage', 'power', 'medium_latency']\n",
    "# Retrieve all files in the output folder\n",
    "file_csv = [file for file in os.listdir(PATH_TO_CSV) if file.endswith('.csv')]\n",
    "file_csv.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in file_csv:\n",
    "    file_path = os.path.join(PATH_TO_CSV, file)\n",
    "    # Read CSV file\n",
    "    df_file = pd.read_csv(file_path)\n",
    "    # Add the current DataFrame to the main DataFrame\n",
    "    df = pd.concat([df, df_file], ignore_index=True)\n",
    "\n",
    "# Delete 'name' column\n",
    "df_no_name = df.drop(columns='name', errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMax scaling to the DataFrame\n",
    "df_no_name_scaled = scaler.fit_transform(df_no_name)\n",
    "\n",
    "# Create a scaled DataFrame with the same columns\n",
    "df_one_function_scaled = pd.DataFrame(df_no_name_scaled, columns=df_no_name.columns)\n",
    "\n",
    "df_one_function_scaled['name'] = df['name']       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_line_plots(df):\n",
    "    for col in df.columns:\n",
    "        if col not in ['rate', 'name']:  \n",
    "            plt.figure(figsize=(10, 6))  \n",
    "            for name in FUNCTION_NAMES:\n",
    "                temp_df = df[df['name'] == name].copy()  \n",
    "                if col == 'medium_latency':\n",
    "                    temp_df[col] = temp_df[col] / 1e9  \n",
    "                if col == 'ram_usage':\n",
    "                    temp_df[col] = temp_df[col] / 1e6  \n",
    "                if col == 'success_rate':\n",
    "                    temp_df[col] = temp_df[col] * 100  \n",
    "                plt.plot(temp_df['rate'], temp_df[col], label=name, marker='o', linestyle='-')\n",
    "            \n",
    "            unit = {\n",
    "                'success_rate': '%',  \n",
    "                'cpu_usage': '%',\n",
    "                'ram_usage': 'MB',  \n",
    "                'power_usage': 'Î¼W',\n",
    "                'overloaded': '',  \n",
    "                'medium_latency': 's'  \n",
    "            }[col]\n",
    "\n",
    "            plt.title(f'{col.capitalize()} per differenti funzioni')\n",
    "            plt.xlabel('Rate (req/s)')\n",
    "            plt.ylabel(f'{col.capitalize()} ({unit})')\n",
    "            \n",
    "            if col == 'overloaded':\n",
    "                plt.yticks([0, 1])\n",
    "\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_line_plots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame according to the column \"name\"\n",
    "data_per_function = {}\n",
    "for name, group in df_one_function_scaled.groupby('name'):\n",
    "    data_per_function[name] = group\n",
    "\n",
    "# Loop through each function's DataFrame in the dictionary\n",
    "for function_name, function_data in data_per_function.items():\n",
    "\n",
    "    # Remove the \"name\" column from each DataFrame in the dictionary\n",
    "    function_data = function_data.drop(columns=['name'])\n",
    "    \n",
    "    # Add the prefix \"avg_\" to each column\n",
    "    function_data.columns = 'avg_' + function_data.columns\n",
    "\n",
    "    # Calculate the average of the values for each group with the same 'rate'\n",
    "    compressed_data = function_data.groupby('avg_rate').mean().reset_index()\n",
    "\n",
    "    # Calculate the maximum values for each group of three rows\n",
    "    max_data = function_data.groupby('avg_rate').max().reset_index()\n",
    "\n",
    "    # Calculate the minimum values for each group of three rows\n",
    "    min_data = function_data.groupby('avg_rate').min().reset_index()\n",
    "\n",
    "    # Add the \"max_\" columns to the compressed DataFrame\n",
    "    compressed_data['max_success_rate'] = max_data['avg_success_rate']\n",
    "    compressed_data['max_cpu_usage'] = max_data['avg_cpu_usage']\n",
    "    compressed_data['max_ram_usage'] = max_data['avg_ram_usage']\n",
    "    compressed_data['max_power'] = max_data['avg_power_usage']\n",
    "    compressed_data['max_overloaded'] = max_data['avg_overloaded']\n",
    "    compressed_data['max_medium_latency'] = max_data['avg_medium_latency']\n",
    "\n",
    "    # Add the \"min_\" columns to the compressed DataFrame\n",
    "    compressed_data['min_success_rate'] = min_data['avg_success_rate']\n",
    "    compressed_data['min_cpu_usage'] = min_data['avg_cpu_usage']\n",
    "    compressed_data['min_ram_usage'] = min_data['avg_ram_usage']\n",
    "    compressed_data['min_power'] = min_data['avg_power_usage']\n",
    "    compressed_data['min_overloaded'] = min_data['avg_overloaded']\n",
    "    compressed_data['min_medium_latency'] = min_data['avg_medium_latency']\n",
    "\n",
    "    compressed_data = compressed_data.drop(columns='avg_rate')\n",
    "\n",
    "    data_per_function[function_name] = compressed_data\n",
    "\n",
    "# Create one DataFrame for each function\n",
    "\n",
    "# Initialize a dictionary to store vectors for each functions\n",
    "vectors_per_function = {}\n",
    "\n",
    "# Loop through each DataFrame function in the dictionary\n",
    "for function_name, function_data in data_per_function.items():\n",
    "    # Concatenates all rows in the DataFrame into a vector\n",
    "    vector = function_data.to_numpy().flatten()\n",
    "    # Saves the vector in the dictionary with the function name as the key\n",
    "    vectors_per_function[function_name] = vector\n",
    "\n",
    "# Trasform Numpy Array to DataFrame pandas\n",
    "\n",
    "# Initialize a dictionary to store DataFrames for each function\n",
    "dataframes_per_function = {}\n",
    "\n",
    "# Loop through each vector in the dictionary\n",
    "for function_name, vector in vectors_per_function.items():\n",
    "    # Create a DataFrame from the vector\n",
    "    dataframe = pd.DataFrame(vector)\n",
    "\n",
    "    # Save the DataFrame in the new dictionary with the function name as the key. Traspose Dataframe\n",
    "    dataframes_per_function[function_name] = dataframe.T\n",
    "\n",
    "# Create functions Dataframe\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "all_dataframes = []\n",
    "function_order = []\n",
    "# Loop through the DataFrames in dataframes_per_function and add them to the list\n",
    "for function_name, dataframe in dataframes_per_function.items():\n",
    "    all_dataframes.append(dataframe)\n",
    "    function_order.append(function_name)\n",
    "\n",
    "# Concatenate all DataFrames in the list to create a single one\n",
    "combined_dataframe = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Scale\n",
    "\n",
    "fill_NaN(combined_dataframe)\n",
    "\n",
    "# Calculate cosine distance for all the vectors representing the functions\n",
    "df_cosine=pd.DataFrame(cosine_similarity(combined_dataframe), columns=function_order)\n",
    "\n",
    "# Create a PCA model\n",
    "pca = PCA()\n",
    "\n",
    "# Apply PCA to the df_functions DataFrame\n",
    "pca_results = pca.fit_transform(combined_dataframe)\n",
    "\n",
    "# Number of dimension for PCA\n",
    "pca_dimensions = len(combined_dataframe.index)\n",
    "\n",
    "# Create a new DataFrame to store the PCA results\n",
    "pca_df = pd.DataFrame(data=pca_results, columns=[f'PC{i}' for i in range(1, pca_dimensions + 1)])\n",
    "\n",
    "# Get the explained variance ratios\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)\n",
    "\n",
    "plt.plot(range(1, pca_dimensions + 1),\n",
    "             explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Numero di Componenti')\n",
    "plt.ylabel('Varianza')\n",
    "plt.show()\n",
    "\n",
    "summed_variance = 0\n",
    "index = 0\n",
    "while summed_variance <= 0.95:\n",
    "    summed_variance += explained_variance[index]\n",
    "    index += 1\n",
    "after_column = \"PC\" + str(index)\n",
    "pca_df = pca_df.truncate(before=\"PC1\", after=after_column, axis=\"columns\")\n",
    "# Save PCA model\n",
    "joblib.dump(pca, 'pca_model.joblib')\n",
    "\n",
    "# Calculate cosine distance for all the pca vectors representing the functions\n",
    "df_cosine_pca=pd.DataFrame(cosine_similarity(pca_df), columns=function_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search the best K params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "inertias = []\n",
    "silhouette_scores = {}\n",
    "K = range(1,7)\n",
    "for k in K:\n",
    "    model_kmeans_k = KMeans(n_clusters = k)\n",
    "    model_kmeans_k.fit(pca_df)\n",
    "    distortions.append(sum(np.min(cdist(pca_df, model_kmeans_k.cluster_centers_, 'euclidean'), axis=1)) / pca_df.shape[0])\n",
    "    inertias.append(model_kmeans_k.inertia_)\n",
    "\n",
    "plot_elbow(K, distortions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply K-Means and show the tabular results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-Means model\n",
    "kmeans_model = KMeans(n_clusters = 3)\n",
    "kmeans_model.fit(pca_df)\n",
    "pca_df['function_name'] = function_order\n",
    "pca_df['cluster'] = kmeans_model.labels_\n",
    "print(pca_df)\n",
    "joblib.dump(kmeans_model, \"profiling-model.joblib\")\n",
    "grouped_df = pca_df.groupby('cluster')['function_name'].apply(list).reset_index()\n",
    "result_dict = dict(zip(grouped_df['cluster'], grouped_df['function_name']))\n",
    "file_path = 'group_list.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(result_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
