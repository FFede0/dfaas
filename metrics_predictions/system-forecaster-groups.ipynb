{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_pinball_loss, r2_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import lightgbm as lgb\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CVS = 'output/output-energy/'\n",
    "GROUP_FILE_PATH = 'group_list.json'\n",
    "# This constant contains the unique quantile to represent in the scatter plot of residuals \n",
    "ONLY_QUANTILE = 0.95\n",
    "# Constant containing the partial name of all the metric of a function\n",
    "FUNCTION_COLUMNS = ['rate_function_', 'success_rate_function_', 'cpu_usage_function_', 'ram_usage_function_', 'power_usage_function_', 'replica_', 'overloaded_function_', 'medium_latency_function_']\n",
    "# A selection of node metrics to represent in the boxplot and in the scatter plot \n",
    "COL_TO_PLOT = ['cpu_usage_idle_node', 'cpu_usage_node', 'ram_usage_idle_node', 'ram_usage_node', 'power_usage_idle_node', 'power_usage_node']\n",
    "# List containing the values whose quantile regression is to be calculated\n",
    "QUANTILES = [0.05, 0.95]\n",
    "NODE_TYPES = [\"LIGHT\", \"MID\", \"HEAVY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to fill NaN values within the dataframe X\n",
    "def fill_NaN(X):\n",
    "  for col in X:\n",
    "    if(col.startswith('success_rate_')):\n",
    "      X.loc[:, col] = X.loc[:, col].fillna(1)\n",
    "    else:\n",
    "      X.loc[:, col] = X.loc[:, col].fillna(0)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reweight of dataframe\n",
    "def resample_dataset(X, y):\n",
    "  X_resampled, y_resampled = resample(X, y, replace=True, random_state=42)\n",
    "  return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to split the dataset into training and test set\n",
    "def split_dataset(X, y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(X_train, X_test, y_train, y_test):\n",
    "  X_train_nn = X_train.astype(np.float32)\n",
    "  X_test_nn = X_test.astype(np.float32)\n",
    "  y_train_nn = y_train.astype(np.float32)\n",
    "  y_test_nn = y_test.astype(np.float32)\n",
    "  \n",
    "  return X_train_nn, X_test_nn, y_train_nn, y_test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate the weighted mean squared error\n",
    "def wmse_score(y_true, y_pred):\n",
    "  # Calculates the weight of classes for the first target  \n",
    "  median_cpu = y_true['cpu_usage_node'].median()\n",
    "  w_majority_cpu = y_true[y_true['cpu_usage_node'] <= median_cpu].shape[0] / y_true.shape[0]\n",
    "  w_minority_cpu = y_true[y_true['cpu_usage_node'] > median_cpu].shape[0] / y_true.shape[0]\n",
    "\n",
    "  # Calculate the weight of classes for the second target\n",
    "  median_ram = y_true['ram_usage_node'].median()\n",
    "  w_majority_ram = y_true[y_true['ram_usage_node'] <= median_ram].shape[0] / y_true.shape[0]\n",
    "  w_minority_ram = y_true[y_true['ram_usage_node'] > median_ram].shape[0] / y_true.shape[0]\n",
    "\n",
    "  # Calculates the MSE for both targets\n",
    "  mse_cpu = mean_squared_error(y_true['cpu_usage_node'], y_pred['cpu_usage_node'])\n",
    "  mse_ram = mean_squared_error(y_true['ram_usage_node'], y_pred['ram_usage_node'])\n",
    "\n",
    "  # Calculates WMSE as a weighted average of the MSEs for the two targets\n",
    "  wmse = (w_majority_cpu * mse_cpu * y_true.shape[0] / (w_majority_cpu * y_true[y_true['cpu_usage_node'] <= median_cpu].shape[0] + w_minority_cpu * y_true[y_true['cpu_usage_node'] > median_cpu].shape[0]) +\n",
    "          w_majority_ram * mse_ram * y_true.shape[0] / (w_majority_ram * y_true[y_true['ram_usage_node'] <= median_ram].shape[0] + w_minority_ram * y_true[y_true['ram_usage_node'] > median_ram].shape[0])) / 2\n",
    "\n",
    "  return wmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate metrics based on the task\n",
    "def metrics(task_type, y_test, y_pred, quantile):\n",
    "  if(task_type == 'regression'):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"mse:\", mse)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print(\"rmse:\", rmse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"R-squared score:\", r2)\n",
    "    std_dev = np.std(y_pred)\n",
    "    print(\"Standard deviation:\", std_dev)\n",
    "    if quantile != 0:\n",
    "      quantile_loss = mean_pinball_loss(y_test, y_pred, alpha=quantile)\n",
    "      print(\"Quantile loss with library: \", quantile_loss)\n",
    "    else:\n",
    "      quantile_loss = \"Not calculated for this target\"\n",
    "    return mse, rmse, r2, quantile_loss, std_dev   \n",
    "  elif(task_type.endswith('classification')):\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to plot the regression lines for the 2 targets\n",
    "def plot_regression(y_test, y_pred, target_name):\n",
    "  # Calculate the regression lines\n",
    "  m, q = np.polyfit(y_test.ravel(), y_pred.ravel(), 1)\n",
    "\n",
    "  # Plot the regression lines\n",
    "  plt.plot(y_test, y_pred, 'o', color='red', fillstyle='none')\n",
    "  plt.plot(y_test, m*y_test + q, linestyle='--')\n",
    "  plt.xlabel('Valori osservati')\n",
    "  plt.ylabel('Valori predetti')\n",
    "  plt.title('Regressione di ' + target_name)\n",
    "  plt.xlim(0, 1) \n",
    "  plt.ylim(0, 1)  \n",
    "  plt.show()\n",
    "\n",
    "  # Calculate residuals\n",
    "  residuals = y_test.flatten() - y_pred.flatten()\n",
    "\n",
    "  # Scatter plot with regression line\n",
    "  sns.scatterplot(x=y_test.flatten(), y=residuals, label=f'Osservazioni')\n",
    "\n",
    "  # Add the horizontal line near the value 0.0 of the y-axis\n",
    "  plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "  plt.xlabel('Valori osservati')\n",
    "  plt.ylabel('Residui (Valori osservati - Valori predetti)')\n",
    "  plt.title(f'Regressione Standard - {target_name}')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to plots quantile regression results.\n",
    "def plot_quantile_regression(y_test, y_pred, target_name):    \n",
    "    point = 1\n",
    "    line = 3\n",
    "    for col in y_pred:\n",
    "        m, q = np.polyfit(y_test.ravel(), y_pred[col], 1)\n",
    "        plt.plot(y_test, y_pred[col], 'o', fillstyle='none', label=f'Osservazioni - {col}')\n",
    "        plt.plot(y_test, m*y_test + q, linestyle='--', zorder = line)\n",
    "        point = point + 1\n",
    "        line = line + 1\n",
    "    plt.xlabel('Valori osservati')\n",
    "    plt.ylabel('Valori predetti')\n",
    "    plt.title('Regressione Quantile di ' + target_name)\n",
    "    plt.xlim(0, 1)  \n",
    "    plt.ylim(0, 1)  \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate quantile residuals\n",
    "    residuals = y_test.flatten() - y_pred[str(ONLY_QUANTILE)]\n",
    "\n",
    "    # Scatter plot with regression line\n",
    "    sns.scatterplot(x=y_test.flatten(), y=residuals, label=f'Osservazioni')\n",
    "\n",
    "    # Add the horizontal line near the value 0.0 of the y-axis\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Valori osservati')\n",
    "    plt.ylabel('Residui quantili (Valori osservati - Valori predetti)')\n",
    "    plt.title(f'Regressione Quantile {ONLY_QUANTILE} - {target_name}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, target):\n",
    "  # Calculate the confusion matrix\n",
    "  cm = confusion_matrix(y_test[target], y_pred[target])\n",
    "\n",
    "  # Plot the confusion matrix as heatmap\n",
    "  sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "  plt.xlabel('Valori osservati')\n",
    "  plt.ylabel('Valori predetti')\n",
    "  plt.title('Confusion matrix')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to train the desired model for a target variable\n",
    "def train_model(target_name, X_train, y_train, quantile):\n",
    "    # Classification task. Create model with autogluon\n",
    "    if target_name.startswith('overloaded') or target_name.startswith('replica'):\n",
    "        model_type = \"\"\n",
    "        model = lgb.LGBMClassifier()\n",
    "    # Quantile regression with LGBM\n",
    "    elif quantile != 0:\n",
    "        model = lgb.LGBMRegressor(objective='quantile', alpha=quantile)\n",
    "        model_type = \"quantile\" + str(quantile).replace('.', '')\n",
    "    # Regression with LGBM\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(objective='regression')\n",
    "        model_type = \"regression\"\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    path = './system-forecaster-models/groups/' + target_name + \"/\" + model_type\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    joblib.dump(model, path + \"/model.joblib\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain a barchart of the type of nodes\n",
    "def plot_node_type_distribution(df):\n",
    "    \"\"\"\n",
    "    Visualizza il numero di dati per ogni tipologia di nodo in un DataFrame tramite un bar chart.\n",
    "\n",
    "    :param df: DataFrame contenente la colonna 'node_type' con le tipologie di nodo.\n",
    "    \"\"\"\n",
    "    # Count of occurrences for each node type\n",
    "    node_counts = df['node_type'].value_counts()\n",
    "\n",
    "    # Bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    node_counts.plot(kind='bar')\n",
    "    plt.title('Distribuzione del Numero di Dati per Tipologia di Nodo')\n",
    "    plt.xlabel('Tipologia di Nodo (0 = Heavy, 1 = Mid, 2 = Light)')\n",
    "    plt.ylabel('Numero di Righe')\n",
    "    plt.xticks(rotation=0)  # Maintains names of horizontal node types\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain a barchart of overloaded node distribution\n",
    "def plot_overloaded_node_distribution(df):\n",
    "    \"\"\"\n",
    "    Visualizza il numero di righe in cui 'overloaded_node' è 1 e quelle in cui è 0.\n",
    "\n",
    "    :param df: DataFrame contenente la colonna 'overloaded_node'.\n",
    "    \"\"\"\n",
    "    # Count of occurrences of 0 and 1 in the 'overloaded_node' column\n",
    "    overloaded_counts = df['overloaded_node'].value_counts()\n",
    "\n",
    "    # Bar chart\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    overloaded_counts.plot(kind='bar')\n",
    "    plt.title('Distribuzione di Nodi Sovraccaricati')\n",
    "    plt.xlabel('Stato Sovraccarico (0 = No, 1 = Sì)')\n",
    "    plt.ylabel('Numero di Righe')\n",
    "    plt.xticks(rotation=0)  \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for node_type in NODE_TYPES:\n",
    "    # Retrieve all files in the output folder\n",
    "    file_csv = [file for file in os.listdir(PATH_TO_CVS + node_type) if file.endswith('.csv')]\n",
    "    # Create the dataframe by concatenating all read files\n",
    "    dataframes = []\n",
    "    for file in file_csv:\n",
    "        file_path = os.path.join(PATH_TO_CVS + node_type, file)\n",
    "        df_temp = pd.read_csv(file_path)       \n",
    "        # Remove the columns in the dataframe that begin with \"function_\"\n",
    "        df_temp.drop(columns=[col for col in df_temp if col.startswith('function_')], inplace=True)        \n",
    "        # Aggiungi la colonna \"node_type\" e assegna il valore di 'type' a tutte le righe\n",
    "        if node_type == \"HEAVY\":\n",
    "            df_temp[\"node_type\"] = 0\n",
    "        elif node_type == \"MID\":\n",
    "            df_temp[\"node_type\"] = 1\n",
    "        else: \n",
    "            df_temp[\"node_type\"] = 2\n",
    "        \n",
    "        dataframes.append(df_temp)\n",
    "\n",
    "    df = pd.concat([df, *dataframes], axis=0, ignore_index=True)\n",
    "\n",
    "df = fill_NaN(df)\n",
    "print(df[\"node_type\"].value_counts())\n",
    "\n",
    "functions = [col[14:] for col in df if col.startswith('rate')]\n",
    "\n",
    "for function in functions:\n",
    "    df.loc[df['rate_function_' + function] == 0, ['cpu_usage_function_' + function, 'ram_usage_function_' + function, 'power_usage_function_' + function, 'replica_' + function]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot node type distribution\n",
    "plot_node_type_distribution(df)\n",
    "# Plot overloaded node distribution\n",
    "plot_overloaded_node_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each target column and handle outliers\n",
    "functions_column = [col for col in df if col.startswith('rate')]\n",
    "targets = [col for col in df if (col.startswith('power_usage_') or col.startswith('cpu_usage_') or col.startswith('ram_usage_') or col.startswith('overloaded_node') or col.startswith('medium_latency')) and 'idle' not in col]\n",
    "grouped = df.groupby(functions_column + ['node_type'])\n",
    "threshold = 1\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    if target != 'overloaded_node':\n",
    "        mean = grouped[target].transform('mean')\n",
    "        std = grouped[target].transform('std')\n",
    "        outliers = (df[target] > mean + threshold * std) | (df[target] < mean - threshold * std)\n",
    "        print(outliers.sum())\n",
    "        df[target] = df[target].where(~outliers, mean)\n",
    "    else:\n",
    "        new_overloaded = grouped[target].transform('all')\n",
    "        df['overloaded_node'] = new_overloaded.astype(int)\n",
    "        print(df[\"overloaded_node\"].value_counts())\n",
    "df_only_useful = df[functions_column + targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the groups definition from the group_list.json \n",
    "with open(GROUP_FILE_PATH, 'r') as json_file:\n",
    "    groups_number = json.load(json_file)\n",
    "\n",
    "# Map groups number with the correspondent name\n",
    "groups = {}\n",
    "for key, value in groups_number.items():\n",
    "    if \"figlet\" in value:\n",
    "        groups[\"LOW_USAGE\"] = value\n",
    "    elif \"nmap\" in value:\n",
    "        groups[\"HIGH_USAGE\"] = value\n",
    "    else:\n",
    "        groups[\"MEDIUM_USAGE\"] = value\n",
    "\n",
    "df_groups = pd.DataFrame()\n",
    "columns = {}\n",
    "for key, group in groups.items():\n",
    "    for metric in FUNCTION_COLUMNS:\n",
    "        temp_columns = [metric + fun for fun in group]\n",
    "        col_name = metric.replace('function_', '')\n",
    "\n",
    "        # Reaname the column name\n",
    "        if col_name == 'rate_':\n",
    "            col_name = 'rate_group_'\n",
    "\n",
    "        # Creates a dataset where the 0 values in the selected columns are replaced with NaN\n",
    "        df_no_zeros = df[temp_columns].mask(df[temp_columns] == 0)\n",
    "        if (metric.__contains__('rate') or metric.__contains__('usage') or metric.__contains__('power') or metric.__contains__('replica')) and not metric.__contains__('success'):\n",
    "            df_groups[col_name + key] = df[temp_columns].sum(axis=1)\n",
    "        elif metric.__contains__('overloaded'):\n",
    "            df_groups[col_name + key] = df[temp_columns].any(axis=1).astype(int)\n",
    "        else:\n",
    "            df_groups[col_name + key] = df_no_zeros.mean(axis=1)\n",
    "\n",
    "df_groups = fill_NaN(df_groups)\n",
    "\n",
    "# Gets all the node columns names present in df\n",
    "node_metrics = [col for col in df if col.endswith('node') or 'node_type' in col] \n",
    "\n",
    "# Copy all the node columns in df_groups \n",
    "for metric in node_metrics:\n",
    "    df_groups[metric] = df[metric]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning into features and targets and oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe division by features and output\n",
    "targets = [col for col in df_groups if (col.startswith('power_usage_') or col.startswith('cpu_usage_') or col.startswith('ram_usage_') or col.startswith('overloaded_node') or col.startswith('medium_latency')) and 'idle' not in col]# or col.startswith('replica')\n",
    "params = [col for col in df_groups if col.startswith('rate_') or 'node_type' in col]\n",
    "\n",
    "# Initialize a dictionary to store target datasets\n",
    "target_datasets = {}\n",
    "features_datasets = {}\n",
    "features_datasets['original'] = df_groups[params]\n",
    "for target_name in targets:\n",
    "    X = df_groups[params]\n",
    "    y = df_groups[[target_name]]\n",
    "    node_type_index = X.columns.get_loc(\"node_type\")\n",
    "\n",
    "    if \"overloaded\" in target_name:\n",
    "        print(\"Status of target: \" + target_name)\n",
    "        print(y.value_counts())\n",
    "        # Oversampling\n",
    "        sm = SMOTENC(random_state=42, categorical_features=[node_type_index])\n",
    "        try:\n",
    "            X_res, y_res = sm.fit_resample(X, y)\n",
    "            result_df = pd.concat([X_res, y_res], axis=1)\n",
    "            print(\"Status after SMOTE:\")\n",
    "            print(y_res.value_counts())\n",
    "            print(X_res[\"node_type\"].value_counts())\n",
    "            features_datasets[target_name] = X_res\n",
    "            y = y_res\n",
    "        except:\n",
    "            print(\"It was not possible to perform SMOTE for target \" + target_name)\n",
    "\n",
    "    target_datasets[target_name] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store scaled data and train-test splits\n",
    "x_train_dict = {}\n",
    "x_test_dict = {}\n",
    "y_train_dict = {}\n",
    "y_test_dict = {}\n",
    "y_scalers = {}\n",
    "scaler_exist = False\n",
    "\n",
    "for target_name in targets:\n",
    "    # Get the target dataset for the current iteration\n",
    "    target_dataset = target_datasets[target_name]\n",
    "\n",
    "    # Apply scaling for x (features)\n",
    "    if \"overloaded\" in target_name:\n",
    "        X = features_datasets[target_name]\n",
    "    else:\n",
    "        X = features_datasets['original']\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(X)\n",
    "    X_scaled = scaler_x.transform(X)\n",
    "\n",
    "    # Apply scaling for y (target)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(target_dataset)\n",
    "    y_scalers[target_name] = scaler_y\n",
    "    if target_name.startswith('overloaded') or target_name.startswith('replica'):\n",
    "        y_scaled = target_dataset\n",
    "    else:\n",
    "        y_scaled = scaler_y.transform(target_dataset)\n",
    "\n",
    "    # Save the scaler for x\n",
    "    scaler_x_path = './scalers/groups/scaler_x/' \n",
    "    if not os.path.exists(scaler_x_path):\n",
    "        os.makedirs(scaler_x_path)\n",
    "    if target_name.startswith('overloaded'):\n",
    "        joblib.dump(scaler_x, scaler_x_path + \"/\" + target_name + \".joblib\")\n",
    "    elif not scaler_exist:\n",
    "        scaler_exist = True\n",
    "        joblib.dump(scaler_x, scaler_x_path + \"/features.joblib\")\n",
    "\n",
    "    # Save the scaler for y\n",
    "    scaler_y_path = './scalers/groups/scaler_y/'\n",
    "    if not os.path.exists(scaler_y_path):\n",
    "        os.makedirs(scaler_y_path)\n",
    "    if not target_name.startswith(\"overloaded\"):\n",
    "        joblib.dump(scaler_y, scaler_y_path + \"/\" + target_name + \".joblib\")\n",
    "\n",
    "    # Split the dataset into training and testing\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n",
    "    x_train_dict[target_name] = x_train\n",
    "    x_test_dict[target_name] = x_test\n",
    "    y_train_dict[target_name] = y_train\n",
    "    y_test_dict[target_name] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Model for each target variable\n",
    "#Initialize a dictionary to store models\n",
    "trained_models = {}\n",
    "for target_name in targets:\n",
    "    #Get the corresponding x_train for the current target\n",
    "    x_train = x_train_dict[target_name]\n",
    "\n",
    "    # Get the corresponding y_train for the current target\n",
    "    y_train = y_train_dict[target_name]\n",
    "\n",
    "    #Trains the specific model for the current target\n",
    "    model = train_model(target_name, x_train, y_train, 0)  # Use the specific target's y_train\n",
    "    trained_models[target_name] = model\n",
    "    if not(target_name.startswith('overloaded') or target_name.startswith('replica')):\n",
    "        for quantile in QUANTILES:\n",
    "            model = train_model(target_name, x_train, y_train, quantile)\n",
    "            trained_models[target_name + \" \" + str(quantile)] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store predictions\n",
    "all_predictions = {}  \n",
    "for target_name in targets:\n",
    "\n",
    "    # Get the corresponding x_train for the current target\n",
    "    x_test = x_test_dict[target_name]\n",
    "\n",
    "    # Get the test dataset for the current iteration\n",
    "    y_test = y_test_dict[target_name]\n",
    "\n",
    "    # Train model\n",
    "    test_data = pd.DataFrame(np.column_stack((x_test, y_test)), columns=[*params, target_name])\n",
    "\n",
    "    model = trained_models[target_name]\n",
    "    y_pred = model.predict(test_data.drop(columns=[target_name]))\n",
    "    all_predictions[target_name] = y_pred\n",
    "    if not(target_name.startswith('overloaded') or target_name.startswith('replica')):\n",
    "        y_pred_quantiles = {}\n",
    "        for quantile in QUANTILES:\n",
    "            model = trained_models[target_name + \" \" + str(quantile)]\n",
    "            y_pred_quantiles[str(quantile)] = model.predict(test_data.drop(columns=[target_name]))\n",
    "        all_predictions[target_name + \" quantiles\"] = y_pred_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_string = [str(q) for q in QUANTILES]\n",
    "\n",
    "for target_name, y_pred in all_predictions.items():\n",
    "    print(f\"Target: {target_name}\")\n",
    "\n",
    "    if target_name.startswith('overloaded'):\n",
    "        task_type = 'binary classification'\n",
    "    elif target_name.startswith('replica'):\n",
    "        task_type = 'classification'\n",
    "    else:\n",
    "        task_type = 'regression'\n",
    "\n",
    "    if target_name.endswith('quantiles'):\n",
    "        # Get the test dataset for the current iteration\n",
    "        trunc_target = target_name[0:target_name.index(' ')]\n",
    "        y_test = y_test_dict[trunc_target]\n",
    "        y_test_df = pd.DataFrame(y_test, columns=[trunc_target])\n",
    "        y_pred_df = pd.DataFrame(y_pred, columns=quantiles_string)\n",
    "        for quantile in QUANTILES:\n",
    "            statistics = (y_pred_df[str(quantile)] > y_test_df[trunc_target]).value_counts(normalize=True)\n",
    "            if statistics.size > 1:\n",
    "                true_percentage, false_percentage = statistics[True], statistics[False]\n",
    "                print(f\"Prediction for alpha {quantile} are greater than true values in {true_percentage * 100} % of cases and less or equal in {false_percentage * 100} % of cases.\")\n",
    "            metrics(task_type, y_test_df, y_pred_df[str(quantile)], quantile) \n",
    "        plot_quantile_regression(y_test, y_pred_df, trunc_target)            \n",
    "    else:\n",
    "        y_test = y_test_dict[target_name]\n",
    "        y_test_df = pd.DataFrame(y_test, columns=[target_name])\n",
    "        y_pred_df = pd.DataFrame(y_pred, columns=[target_name])\n",
    "        metrics(task_type, y_test_df, y_pred_df, 0)\n",
    "        if task_type == 'regression':\n",
    "            plot_regression(y_test, y_pred, target_name)   \n",
    "\n",
    "    print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
