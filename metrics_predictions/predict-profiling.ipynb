{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_TO_PREDICT = 'figlet'\n",
    "PATH_TO_CSV = 'output/generator'\n",
    "# Retrieve all files in the output folder\n",
    "file_csv = [file for file in os.listdir(PATH_TO_CSV) if file.endswith('.csv')]\n",
    "file_csv.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in file_csv:\n",
    "    if FUNCTION_TO_PREDICT in file:\n",
    "        file_path = os.path.join(PATH_TO_CSV, file)\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "# Delete 'name' column\n",
    "df_no_name = df.drop(columns='name', errors='ignore')\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMax scaling to the DataFrame\n",
    "df_no_name_scaled = scaler.fit_transform(df_no_name)\n",
    "\n",
    "# Create a scaled DataFrame with the same columns\n",
    "df_one_function_scaled = pd.DataFrame(df_no_name_scaled, columns=df_no_name.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the prefix \"avg_\" to each column\n",
    "df_one_function_scaled.columns = 'avg_' + df_one_function_scaled.columns\n",
    "\n",
    "# Calculate the average of the values for each group with the same 'rate'\n",
    "compressed_data = df_one_function_scaled.groupby('avg_rate').mean().reset_index()\n",
    "\n",
    "# Calculate the maximum values for each group of three rows\n",
    "max_data = df_one_function_scaled.groupby('avg_rate').max().reset_index()\n",
    "\n",
    "# Calculate the minimum values for each group of three rows\n",
    "min_data = df_one_function_scaled.groupby('avg_rate').min().reset_index()\n",
    "\n",
    "# Add the \"max_\" columns to the compressed DataFrame\n",
    "compressed_data['max_success_rate'] = max_data['avg_success_rate']\n",
    "compressed_data['max_cpu_usage'] = max_data['avg_cpu_usage']\n",
    "compressed_data['max_ram_usage'] = max_data['avg_ram_usage']\n",
    "compressed_data['max_power'] = max_data['avg_power_usage']\n",
    "compressed_data['max_overloaded'] = max_data['avg_overloaded']\n",
    "compressed_data['max_medium_latency'] = max_data['avg_medium_latency']\n",
    "\n",
    "# Add the \"min_\" columns to the compressed DataFrame\n",
    "compressed_data['min_success_rate'] = min_data['avg_success_rate']\n",
    "compressed_data['min_cpu_usage'] = min_data['avg_cpu_usage']\n",
    "compressed_data['min_ram_usage'] = min_data['avg_ram_usage']\n",
    "compressed_data['min_power'] = min_data['avg_power_usage']\n",
    "compressed_data['min_overloaded'] = min_data['avg_overloaded']\n",
    "compressed_data['min_medium_latency'] = min_data['avg_medium_latency']\n",
    "\n",
    "compressed_data = compressed_data.drop(columns='avg_rate')\n",
    "\n",
    "# Concatenates all rows in the DataFrame into a vector\n",
    "vector = compressed_data.to_numpy().flatten()\n",
    "\n",
    "# Create a DataFrame from the vector\n",
    "dataframe = pd.DataFrame(vector)\n",
    "\n",
    "# Traspose Dataframe\n",
    "dataframe = dataframe.T\n",
    "\n",
    "# PCA\n",
    "pca_model = joblib.load('pca_model.joblib')\n",
    "pca_results = pca_model.transform(dataframe)\n",
    "\n",
    "# Number of PCA dimensions\n",
    "pca_dimensions = pca_results.shape[1]\n",
    "\n",
    "# Create a DataFrame with the results of the PCA\n",
    "pca_df = pd.DataFrame(data=pca_results, columns=[f'PC{i}' for i in range(1, pca_dimensions + 1)])\n",
    "\n",
    "# Calculates the cumulative variance\n",
    "explained_variance = pca_model.explained_variance_ratio_\n",
    "\n",
    "summed_variance = 0\n",
    "index = 0\n",
    "while summed_variance <= 0.95:\n",
    "    summed_variance += explained_variance[index]\n",
    "    index += 1\n",
    "after_column = \"PC\" + str(index)\n",
    "\n",
    "# Truncate the dataframe based on the size taken with a variance greater than 0.95\n",
    "pca_df = pca_df.truncate(before=\"PC1\", after=after_column, axis=\"columns\")\n",
    "\n",
    "kmeans_model = joblib.load('profiling-model.joblib')\n",
    "kmeans_predict = kmeans_model.predict(pca_df)\n",
    "\n",
    "# Change group_list.json\n",
    "\n",
    "# Extract the group key\n",
    "kmeans_predict_key = tuple(kmeans_predict.tolist())\n",
    "str_key = \",\".join(map(str, kmeans_predict_key))\n",
    "\n",
    "# Load 'group_list.json' file\n",
    "file_path = 'group_list.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    group_list = json.load(file)\n",
    "\n",
    "# Add the new function to the corresponding group\n",
    "if str_key in group_list:\n",
    "    if FUNCTION_TO_PREDICT not in group_list[str_key]:\n",
    "        group_list[str_key].append(FUNCTION_TO_PREDICT)\n",
    "else:\n",
    "    group_list[str_key] = [FUNCTION_TO_PREDICT]\n",
    "\n",
    "# Save the 'group_list.json' file with the changes made\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(group_list, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
